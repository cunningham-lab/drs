{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sample_simulated_data as ssd\n",
    "import drs_utils as drs\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 #number of neurons\n",
    "types_of_trials = 2\n",
    "samples_per_type = 500\n",
    "R = types_of_trials * samples_per_type #total number of trials\n",
    "xs = np.linspace(0,10,101)\n",
    "T = 10.0 #time interval\n",
    "d = 2 #dimension of latent space\n",
    "L = 2 #number of epsilons used for each x_{r,i} in the minibatch\n",
    "batch_size = 2 #batching over trials\n",
    "n_intervals = 10\n",
    "nodes = np.linspace(0.0, T, n_intervals+1, dtype='float32') #spline nodes are fixed here, learning them causes numerical instability\n",
    "p = 3 #degree of polynomial used in each interval\n",
    "s1 = p/2 + 1 #Q1 is s1 x s1\n",
    "s2 = (p + 1)/2 #Q2 is s2 x s2\n",
    "smooth = 2\n",
    "units_mu = 100\n",
    "units_sigma = 100\n",
    "units_f = [100, 100, 100, (s1**2 + s2**2) * n_intervals * N] #last layer should output the number of parameters used to parametrize the splines\n",
    "lr = 0.001 #learning rate\n",
    "alt_steps = 53\n",
    "max_epochs = 4 #number of epochs\n",
    "# logdir = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated data\n",
    "mu = np.zeros(len(xs)) + 0.5\n",
    "sigma2 = 1.0\n",
    "np.random.seed(2480)\n",
    "\n",
    "S_all, mask_all, lengths_all, max_lengths, ys_all = ssd.create_data_set(xs, N, types_of_trials,\n",
    "                                                                        int(1.2*samples_per_type), mu, sigma2, ssd.RBF)\n",
    "aux_indices = np.random.choice(int(R * 1.2), int(R * 1.2), replace=False)\n",
    "train_indices = aux_indices[:R]\n",
    "test_indices = aux_indices[R:]\n",
    "S_tensor = S_all[:,train_indices,:]\n",
    "S_tensor_test = S_all[:,test_indices,:]\n",
    "mask_tensor = mask_all[:,train_indices,:]\n",
    "mask_tensor_test = mask_all[:,test_indices,:]\n",
    "lengths = lengths_all[:, train_indices]\n",
    "lengths_test = lengths_all[:, test_indices]\n",
    "ys = ys_all[:,train_indices,:]\n",
    "ys_test = ys_all[:,test_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary class for obtaining batches\n",
    "class PermManager(object):\n",
    "    def __init__(self, n, batch_size, perm=None, perm_index=0, epoch=0):\n",
    "        assert batch_size <= n\n",
    "        self.n = n\n",
    "        self.batch_size = batch_size\n",
    "        if perm is None:\n",
    "            self.perm = np.random.permutation(self.n)\n",
    "        else:\n",
    "            self.perm = perm\n",
    "        self.perm_index = perm_index\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def get_indices(self):\n",
    "        if self.perm_index + self.batch_size < self.n:\n",
    "            indices = self.perm[self.perm_index:self.perm_index+self.batch_size]\n",
    "            self.perm_index = self.perm_index + self.batch_size\n",
    "        elif self.perm_index + self.batch_size == self.n:\n",
    "            indices = self.perm[self.perm_index:self.perm_index+self.batch_size]\n",
    "            self.perm_index = 0\n",
    "            self.perm = np.random.permutation(self.n)\n",
    "            self.epoch = self.epoch + 1\n",
    "        else:\n",
    "            surplus = self.perm_index + self.batch_size - self.n\n",
    "            indices1 = self.perm[self.perm_index:]\n",
    "            self.perm = np.random.permutation(self.n)\n",
    "            indices2 = self.perm[:surplus]\n",
    "            indices = np.concatenate((indices1, indices2), axis=0)\n",
    "            self.perm_index = surplus\n",
    "            self.epoch = self.epoch + 1\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "eps = tf.placeholder(tf.float32, shape=(None, L, d))\n",
    "X = tf.placeholder(tf.float32, shape=(N, None, max_lengths))\n",
    "lengths_tf = tf.placeholder(tf.int32, shape=(N, None))\n",
    "Y = tf.tile(tf.reshape(X, [N, -1, 1, max_lengths]), [1, 1, L, 1]) # repeat X\n",
    "\n",
    "#transform eps into Z, result should be [batch_size*L, d]\n",
    "mu = drs.mu_rnns(X, units_mu, lengths_tf, N, d) #[batch_size, d]\n",
    "sigma = drs.sigma_rnns(X, units_sigma, lengths_tf, N, d) #[batch_size, d]\n",
    "mu_repeat = tf.reshape(tf.tile(tf.reshape(mu, [-1, 1, d]), [1, L, 1]), [-1, d])\n",
    "sigma_repeat = tf.reshape(tf.tile(tf.reshape(sigma, [-1, 1, d]), [1, L, 1]), [-1, d])\n",
    "Z = mu_repeat + sigma_repeat * tf.reshape(eps, [-1, d])\n",
    "\n",
    "#transform the hidden variable to the parameters of the PP\n",
    "#Q1s has shape [N,batch_size,L,n_intervals,s1,s1] and Q2s [N,batch_size,L,n_intervals,s2,s2]\n",
    "Q1s, Q2s = drs.f_nn(Z, units_f, alt_steps, nodes, n_intervals, p, s1, s2, N, L, smooth)\n",
    "\n",
    "#compute the log density of y|z where x (y) given z (eta) is a PP\n",
    "mask_tf = tf.placeholder(tf.float32, shape=(N, None, max_lengths)) #None here should be batch_size\n",
    "mask_repeat = tf.tile(tf.reshape(mask_tf, [N, -1, 1, max_lengths]), [1, 1, L, 1])\n",
    "log_p = tf.reduce_sum(drs.log_intensities(Y, Q1s, Q2s, nodes, mask_repeat, L, max_lengths, n_intervals, p, s1, s2, N,\n",
    "                                          T, 0.001)) - tf.reduce_sum(drs.integral(Q1s, Q2s, nodes, p, N, L, s1, s2))\n",
    "log_p = R / (batch_size * L) * log_p\n",
    "\n",
    "#compute the KL (variational posterior to prior)\n",
    "neg_KL = 0.5 * tf.reduce_sum(1.0 + 2.0 * tf.log(sigma) - mu**2 - sigma**2)\n",
    "neg_KL = R / batch_size * neg_KL\n",
    "\n",
    "#optimize\n",
    "cost = - (log_p + neg_KL)\n",
    "optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "all_params = tf.trainable_variables()\n",
    "grads_and_vars = optim.compute_gradients(cost, all_params)\n",
    "clipped_grads_and_vars = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in grads_and_vars]\n",
    "optimizer = optim.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "#saver = tf.train.Saver(tf.global_variables())\n",
    "#summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "#summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)\n",
    "\n",
    "#ckpt_state = tf.train.get_checkpoint_state(logdir)\n",
    "#if ckpt_state and ckpt_state.model_checkpoint_path:\n",
    "#    saver.restore(sess, ckpt_state.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def epoch_cost(R_train, batch_size, X_eval, mask_eval, lengths_eval):\n",
    "    assert np.shape(X_eval)[1] % batch_size == 0\n",
    "    negative_ELBO = 0.0\n",
    "    local_perms = PermManager(np.shape(X_eval)[1], batch_size)\n",
    "    while local_perms.epoch < 1:\n",
    "        eps_batch = np.random.normal(size=[batch_size, L, d])\n",
    "        batch = local_perms.get_indices()\n",
    "        X_batch = X_eval[:, batch, :]\n",
    "        mask_batch = mask_eval[:, batch, :]\n",
    "        lengths_batch = lengths_eval[:, batch]\n",
    "        negative_ELBO += sess.run(cost, {eps: eps_batch, X: X_batch, mask_tf: mask_batch, lengths_tf: lengths_batch})\n",
    "    negative_ELBO = negative_ELBO * batch_size / R_train\n",
    "    del local_perms\n",
    "    return negative_ELBO\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "perms = PermManager(R, batch_size)\n",
    "while True:\n",
    "    start_epoch = perms.epoch\n",
    "    eps_batch = np.random.normal(size=[batch_size, L, d])\n",
    "    batch = perms.get_indices()\n",
    "    X_batch = S_tensor[:, batch, :]\n",
    "    mask_batch = mask_tensor[:, batch, :]\n",
    "    lengths_batch = lengths[:, batch]\n",
    "    #_, summary = sess.run([optimizer, summary_op], {eps: eps_batch, X: X_batch, mask_tf: mask_batch, lengths_tf: lengths_batch})\n",
    "    #summary_writer.add_summary(summary, step)\n",
    "    _, c = sess.run([optimizer, cost], {eps: eps_batch, X: X_batch, mask_tf: mask_batch, lengths_tf: lengths_batch})\n",
    "    if perms.epoch > start_epoch:\n",
    "        train_costs.append(epoch_cost(R, batch_size, S_tensor, mask_tensor, lengths))\n",
    "        test_costs.append(epoch_cost(R, batch_size, S_tensor_test, mask_tensor_test, lengths_test))\n",
    "        print('epoch '+str(start_epoch)+' done with cost '+str(train_costs[-1]))\n",
    "    #if step % 1000 == 0 or step + 1 == max_iter:\n",
    "    #    checkpoint_path = os.path.join(logdir, 'model.ckpt')\n",
    "    #    saver.save(sess, checkpoint_path)\n",
    "    if perms.epoch >= max_epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print -test_costs[-1] / 200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_final = sess.run(mu, {X:S_tensor, lengths_tf:lengths})\n",
    "plt.figure(figsize=(5,4))\n",
    "color_idx = np.linspace(0.1,0.9,types_of_trials)\n",
    "for r in xrange(R):\n",
    "    r_type = train_indices[r] % types_of_trials\n",
    "    plt.scatter(mu_final[r,0], mu_final[r,1], color=plt.cm.seismic(color_idx[r_type]), s=0.2)\n",
    "plt.title(\"Variational posterior means colored by trial type (train)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"simulated_spline_posterior_means_train.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = (55.0 / 255.0, 126.0 / 255.0, 184.0 / 255.0)\n",
    "red = (228.0 / 255.0, 26.0 / 255.0, 28.0 / 255.0)\n",
    "orange = (255.0 / 255.0, 127.0 / 255.0, 0.0 / 255.0)\n",
    "yellow = (255.0 / 255.0, 255.0 / 255.0, 51.0 / 255.0)\n",
    "mu_final_test = sess.run(mu, {X:S_tensor_test, lengths_tf:lengths_test})\n",
    "plt.figure(figsize=(5,4))\n",
    "color_list = [orange, yellow]\n",
    "for r in xrange(int(0.2 * R)):\n",
    "    r_type = test_indices[r] % types_of_trials\n",
    "    if r < 2:\n",
    "        plt.scatter(mu_final_test[r,0], mu_final_test[r,1], color=color_list[r_type], label='trial type '+str(r_type+1))\n",
    "    else:\n",
    "        plt.scatter(mu_final_test[r,0], mu_final_test[r,1], color=color_list[r_type])\n",
    "# plt.title(\"Variational posterior means colored by trial type\")\n",
    "plt.legend()\n",
    "plt.xlabel('latent dimension 1')\n",
    "plt.ylabel('latent dimension 2')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"simulated_spline_posterior_means_test_poster.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 0\n",
    "trial = 1\n",
    "S_trial = S_tensor_test[:,[trial],:]\n",
    "lengths_trial = lengths_test[:, [trial]]\n",
    "Q1s_samples_0, Q2s_samples_0 = sess.run([Q1s, Q2s], {X:S_trial, eps:np.random.normal(size=[1, L, d]), lengths_tf:lengths_trial})\n",
    "Q1s_samples_1, Q2s_samples_1 = sess.run([Q1s, Q2s], {X:S_trial, eps:np.random.normal(size=[1, L, d]), lengths_tf:lengths_trial})\n",
    "\n",
    "def compute_spline(xs, Q1s, Q2s, neuron, l, interval, p, r=0):\n",
    "    #only works for quadratic and cubic splines\n",
    "    Q1 = Q1s[neuron, r, l, interval,:,:]\n",
    "    Q2 = Q2s[neuron, r, l, interval,:,:]\n",
    "    if p == 2:\n",
    "        pol1 = Q1[0,0] + (Q1[0,1] + Q1[1,0]) * xs + Q1[1,1] * xs**2\n",
    "        pol2 = Q2[0,0] * (xs - nodes[interval]) * (nodes[interval+1] - xs)\n",
    "    if p == 3:\n",
    "        pol1 = (nodes[interval+1] - xs) * (Q1[0,0] + (Q1[0,1] + Q1[1,0]) * xs + Q1[1,1] * xs**2)\n",
    "        pol2 = (xs - nodes[interval]) * (Q2[0,0] + (Q2[0,1] + Q2[1,0]) * xs + Q2[1,1] * xs**2)\n",
    "    return(pol1+pol2)\n",
    "\n",
    "green = (77.0 / 255.0, 175.0 / 255.0, 74.0 / 255.0)\n",
    "purple = (152.0 / 255.0, 78.0 / 255.0, 163.0 / 255.0)\n",
    "red = (228.0 / 255.0, 26.0 / 255.0, 28.0 / 255.0)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(xs, ys_test[neuron,trial,:], color=green, label=\"true intensity\")\n",
    "plt.scatter(S_tensor_test[neuron,trial,:lengths_test[neuron,trial]], np.zeros(lengths_test[neuron,trial]), color=red, label=\"events\", marker='x')\n",
    "# plt.title(\"Inferred intensity for a simulated spike train\")\n",
    "plt.xlabel('time (unitless)')\n",
    "plt.ylabel('intensity')\n",
    "for i in xrange(n_intervals):\n",
    "    xs_loc = np.linspace(nodes[i], nodes[i+1], 25)\n",
    "    ys_0 = compute_spline(xs_loc, Q1s_samples_0, Q2s_samples_0, neuron, 0, i, p)\n",
    "    if i == 0:\n",
    "        plt.plot(xs_loc, ys_0, color=purple, label=\"DRS-VAE posterior samples\")\n",
    "    else:\n",
    "        plt.plot(xs_loc, ys_0, color=purple)\n",
    "    ys_1 = compute_spline(xs_loc, Q1s_samples_0, Q2s_samples_0, neuron, 1, i, p)\n",
    "    plt.plot(xs_loc, ys_1, color=purple)\n",
    "    ys_2 = compute_spline(xs_loc, Q1s_samples_1, Q2s_samples_1, neuron, 0, i, p)\n",
    "    plt.plot(xs_loc, ys_2, color=purple)\n",
    "    ys_3 = compute_spline(xs_loc, Q1s_samples_1, Q2s_samples_1, neuron, 1, i, p)\n",
    "    plt.plot(xs_loc, ys_3, color=purple)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"simulated_spline_intensities.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(ys_true, ys, xs):\n",
    "    # ys_true and ys have shape [R, N, len(xs)] and each represent RN functions, each evaluated at each point in xs,\n",
    "    # which has shape [len(xs)].\n",
    "    # the output has shape [R, N] and approximates the l2 distance between ys_true and ys for every function.\n",
    "    T = xs[-1] - xs[0]\n",
    "    widths = xs[1:] - xs[:-1]\n",
    "    heights = ((ys_true[:,:,1:] + ys_true[:,:,:-1]) / 2.0 - (ys[:,:,1:] + ys[:,:,:-1]) / 2.0)**2\n",
    "    l2s = np.sqrt(np.sum(np.reshape(widths, [1, 1, -1]) * heights, axis=2))\n",
    "    return(l2s)\n",
    "\n",
    "def compute_all_splines(xs, Q1s, Q2s, l, p):\n",
    "    # returns an array of shape [R, N, len(xs)] where the [r,n,i] entry corresponds to the value of the spline\n",
    "    # of the r-th trial, n-th neuron at xs[i] for sample l (Q1s and Q2s have shape [N, R, L, n_intervals, ?, ?])\n",
    "    N, R, _, n_intervals, _, _ = np.shape(Q1s)\n",
    "    out = []\n",
    "    for n in xrange(N):\n",
    "        for r in xrange(R):\n",
    "            local_out = []\n",
    "            for interval in xrange(n_intervals):\n",
    "                if interval == 0:\n",
    "                    local_xs = xs[np.logical_and(xs>=nodes[interval], xs<=nodes[interval+1])]\n",
    "                else:\n",
    "                    local_xs = xs[np.logical_and(xs>nodes[interval], xs<=nodes[interval+1])]\n",
    "                local_out = np.concatenate((local_out, compute_spline(local_xs, Q1s, Q2s, n, l, interval, p, r)))\n",
    "            out.append(np.reshape(local_out, [-1]))\n",
    "\n",
    "    return(np.transpose(np.reshape(out, [N, R, -1]), [1, 0, 2]))\n",
    "\n",
    "Q1s_samples_test_0, Q2s_samples_test_0 = sess.run([Q1s, Q2s], {X:S_tensor_test, eps:np.random.normal(size=[200, L, d]), lengths_tf:lengths_test})\n",
    "drs_y = compute_all_splines(xs, Q1s_samples_test_0, Q2s_samples_test_0, 0, 3)\n",
    "drs_l2s = l2_distance(np.transpose(ys_test, [1, 0, 2]), drs_y, xs)\n",
    "print 'average l2 distance from posterior samples to truth for DRS: ' + str(np.mean(drs_l2s)) +'+-' + str(np.std(drs_l2s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
